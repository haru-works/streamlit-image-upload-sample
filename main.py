import os
import sys
import asyncio
import time
import base64
import shutil
import streamlit as st
from streamlit_scroll_navigation import scroll_navbar
from PIL import Image
from pydantic import SecretStr
# langchain
from langchain_google_genai import ChatGoogleGenerativeAI 
from langchain_core.messages import HumanMessage,SystemMessage
# video
from moviepy.editor import VideoFileClip
import cv2
from scenedetect import SceneManager
from scenedetect.detectors import ContentDetector
from scenedetect import VideoManager
from scenedetect import SceneManager
import tempfile
import math
# audio
#import whisper


# ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å«ã‚€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¾ã§ãƒ‘ã‚¹ã‚’é€šã™
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# ç’°å¢ƒå¤‰æ•°ã®èª­è¾¼
from dotenv import load_dotenv
load_dotenv()


# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="streamlit demo",layout="wide")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆ
if "base64_data" not in st.session_state:
    st.session_state["base64_data"] = None
if "image_data" not in st.session_state:
    st.session_state["image_data"] = None 
    
if "video_scene_file_list" not in st.session_state:
    st.session_state["video_scene_file_list"] = []
if "tmp_audio_file_path" not in st.session_state:
    st.session_state["tmp_audio_file_path"] = None
    
if "uploaded_pic" in st.session_state and st.session_state["uploaded_pic"]:
    st.toast("ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ", icon="ğŸ“¥")
    del st.session_state["uploaded_pic"]
if "uploaded_video" in st.session_state and st.session_state["uploaded_video"]:
    st.toast("å‹•ç”»ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ", icon="ğŸ“¥")
    del st.session_state["uploaded_video"]
    
if "messages" not in st.session_state:
    st.session_state["messages"] = []
    

if "anchor_ids" not in st.session_state:
    st.session_state["anchor_ids"] = []
if "anchor_icons" not in st.session_state:
    st.session_state["anchor_icons"] = []
if "anchor_labels" not in st.session_state:
    st.session_state["anchor_labels"] = []



# å‰Šé™¤ç”¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°
def delete_scene(target_no):
    if len(st.session_state["video_scene_file_list"]) > 0:
        # æ–°ã—ã„åˆæœŸå€¤ã®ãƒªã‚¹ãƒˆ
        new_values = []
        for j in range(len(st.session_state["video_scene_file_list"])):
            # å‰Šé™¤å¯¾è±¡ã®target_noã¯ã‚¹ã‚­ãƒƒãƒ—
            if j != target_no:
                #print( "å‰Šé™¤å¯¾è±¡ä»¥å¤–ã‚’è¿½åŠ  j:" + str(j))
                # åˆæœŸå€¤ã«ç¾åœ¨ã®å€¤ã‚’è¿½åŠ 
                new_values.append(st.session_state["video_scene_file_list"][j])
            else:
                print("å‰Šé™¤å¯¾è±¡ target_no:" + str(target_no) + "ã‚’é™¤å¤–")     
                os.remove(st.session_state["video_scene_file_list"][j]["video_scene_file_path"])
    
        # ãƒªã‚¹ãƒˆã‚’æ›´æ–°
        st.session_state["video_scene_file_list"] = new_values

  
  
    
# llmåˆæœŸåŒ–
def init_llm(llm_model: str,temperature):
    if llm_model == 'gemini-1.5-flash':
        return ChatGoogleGenerativeAI(model="gemini-1.5-flash",
                                      api_key=SecretStr(os.getenv('GOOGLE_API_KEY')),
                                      temperature=temperature,)
    elif llm_model == 'gemini-2.0-flash':
        return ChatGoogleGenerativeAI(model="gemini-2.0-flash",
                                      api_key=SecretStr(os.getenv('GOOGLE_API_KEY')),
                                      temperature=temperature,)
    else:
        st.error(f'ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ãªã„LLMãƒ¢ãƒ‡ãƒ«ã§ã™: {llm_model}',icon="âœ–")



# ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
@st.dialog("ğŸ¨ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
def upload_image():
    st.warning("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸã„ç”»åƒã‚’é¸æŠã—ã¦ãã ã•ã„",icon="ğŸ’¡",)
    picture = st.file_uploader("ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ", 
                               type=["jpg", "png", "bmp"], 
                               label_visibility="hidden") 
    
    # ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    if picture:   
        
        # ç”»åƒã‚’ãƒã‚¤ãƒŠãƒªã«å¤‰æ›
        bytes_data = picture.getvalue()
        # ç”»åƒã‚’è¡¨ç¤º
        image = Image.open(picture)
        st.session_state["uploaded_pic"] = True
        st.image(image, caption='ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼') 
        
        #ã€€ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³æŠ¼ã™
        if st.button("ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"):
            # ç”»åƒã‚’base64ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            base64_data = base64.b64encode(bytes_data).decode('utf-8')
            st.session_state["base64_data"] = base64_data
            st.session_state["image_data"] = image
            st.rerun()



# å‹•ç”»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
@st.dialog("ğŸ¥å‹•ç”»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
def upload_video():
    st.warning("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸã„å‹•ç”»ã‚’é¸æŠã—ã¦ãã ã•ã„",icon="ğŸ’¡",)
    video = st.file_uploader("ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ", 
                               type=["mp4", "avi"], 
                               label_visibility="hidden") 
    
    # å‹•ç”»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    if video:   
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        with tempfile.NamedTemporaryFile(delete=False, suffix=video.name) as temp_file:
            temp_file.write(video.read())
            temp_file_path = temp_file.name
            # å‹•ç”»ã‚’è¡¨ç¤º
            st.session_state["uploaded_video"] = True
            st.video(video) 
            
            selected_mode = st.radio(label="ãƒ¢ãƒ¼ãƒ‰é¸æŠ",options=["æ‰‹å‹•","è‡ªå‹•"],horizontal=True)
            
            if selected_mode == "æ‰‹å‹•":
                seconds_per_frame = st.number_input(label="ãƒ•ãƒ¬ãƒ¼ãƒ æ•°/ç§’",value=10,step=1) 
            else:
                threshold = st.number_input(label="é–¾å€¤",value=30,step=1) 
                 
            if st.button("ã‚·ãƒ¼ãƒ³åˆ†å‰²"):
                # åˆæœŸåŒ–
                st.session_state["video_scene_file_list"] = []
                st.session_state["tmp_audio_file_path"] = None 
                st.session_state["anchor_ids"] = []
                st.session_state["anchor_labels"] = []
                st.session_state["anchor_icons"] = [] 
                st.session_state["delete_scene_no"] = None
                shutil.rmtree("tmp_mp3/tmp")
                os.mkdir("tmp_mp3/tmp")       
                shutil.rmtree("tmp_video")
                os.mkdir("tmp_video")   
                
                
                if selected_mode == "æ‰‹å‹•":
                    audio_path,json_data_list = split_video_into_scenes_manual(video_path=temp_file_path,seconds_per_frame=seconds_per_frame)
                else:
                    audio_path ,json_data_list = split_video_into_scenes_auto(video_path=temp_file_path,threshold=threshold)

                st.session_state["tmp_audio_file_path"] = audio_path
                st.session_state["video_scene_file_list"] = json_data_list
                st.rerun()        
                


# å‹•ç”»ãƒ•ãƒ¬ãƒ¼ãƒ è§£æ
def split_video_into_scenes_auto(video_path, threshold=27.0):
    with st.spinner("ã‚·ãƒ¼ãƒ³åˆ†å‰²ä¸­ï¼ï¼ï¼",show_time=True):
        # VideoManager ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        video_manager = VideoManager([video_path])
        scene_manager = SceneManager()
        # ãƒ“ãƒ‡ã‚ªã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆã‚’å–å¾—
        framerate = video_manager.get_framerate()
        # ContentDetectorã‚’é–¾å€¤ã€æœ€å°ã‚·ãƒ¼ãƒ³é•·ã€ãƒ•ãƒ¬ãƒ¼ãƒ ã‚¹ã‚­ãƒƒãƒ—ã¨å…±ã«è¿½åŠ 
        scene_manager.add_detector(ContentDetector(threshold))
        # ãƒ“ãƒ‡ã‚ªã¨ã‚·ãƒ¼ãƒ³ãƒãƒãƒ¼ã‚¸ãƒ£ã‚’é–¢é€£ä»˜ã‘
        video_manager.set_downscale_factor()
        video_manager.start()
        scene_manager.detect_scenes(frame_source=video_manager)
        # æ¤œå‡ºã•ã‚ŒãŸã‚·ãƒ¼ãƒ³ã®ãƒªã‚¹ãƒˆã‚’å–å¾—
        scene_list = scene_manager.get_scene_list(base_timecode=video_manager.get_base_timecode(),start_in_scene=True)
        # å„ã‚·ãƒ¼ãƒ³ã®æœ€åˆã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç”»åƒã¨ã—ã¦ä¿å­˜ã—ã€ã‚·ãƒ¼ãƒ³ã®ç§’æ•°ã‚’è¨ˆç®—
        cap = cv2.VideoCapture(video_path)
        json_data_list = []
        base64Frames = []
        for i, scene in enumerate(scene_list):
            start_frame = scene[0].get_frames()
            end_frame = scene[1].get_frames()
            scene_duration = (end_frame - start_frame) / framerate
            procedure = ""
            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
            ret, frame = cap.read()
            
            if ret:
                scene_no = i+1
                img_path = f'tmp_video/scene_{scene_no}.jpg'
                scene_time_start = scene[0].get_timecode()
                scene_time_end = scene[1].get_timecode()
                _, buffer = cv2.imencode(".jpg", frame) # æ–‡å­—åˆ—ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
                base64_data = (base64.b64encode(buffer).decode("utf-8"))
                cv2.imwrite(img_path, frame)
                json_data = {
                    "scene_no":scene_no,
                    "scene_time_start":scene_time_start,
                    "scene_time_end":scene_time_end,
                    "scene_duration":scene_duration,
                    "video_scene_file_path":img_path,
                    "procedure":procedure,
                    "base64_data":base64_data,
                }
                json_data_list.append(json_data)
            cap.release()
            
        # Extract audio from video
        audio_path = f"tmp_mp3/{video_path}.mp3"
        clip = VideoFileClip(video_path)
        clip.audio.write_audiofile(audio_path, bitrate="32k")
        clip.audio.close()
        clip.close()

    print(f"Extracted {len(base64Frames)} frames")
    print(f"Extracted audio to {audio_path}")
    return audio_path, json_data_list



# å‹•ç”»ãƒ•ãƒ¬ãƒ¼ãƒ è§£æ&éŸ³å£°æŠ½å‡º
def split_video_into_scenes_manual(video_path, seconds_per_frame=2):
    
    with st.spinner("ã‚·ãƒ¼ãƒ³åˆ†å‰²ä¸­â€¦",show_time=True):
        
        base_video_path, _ = os.path.splitext(video_path)
        video = cv2.VideoCapture(video_path)
        total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = video.get(cv2.CAP_PROP_FPS)
        print("fps=", fps)
        frames_to_skip = int(fps * seconds_per_frame)
        curr_frame=0
        i=1
        json_data_list = []
        base64Frames = []
        while curr_frame < total_frames - 1:
            scene_no = i
            scene_time_start = ""
            scene_time_end = ""
            scene_duration = ""
            procedure = ""
            img_path = f'tmp_video/scene_{scene_no}.jpg'          
            video.set(cv2.CAP_PROP_POS_FRAMES, curr_frame)
            success, frame = video.read()
            if not success:
                break
            _, buffer = cv2.imencode(".jpg", frame)
            base64_data = base64.b64encode(buffer).decode("utf-8")
            cv2.imwrite(img_path, frame)
            curr_frame += frames_to_skip
            json_data = {
                    "scene_no":scene_no,
                    "scene_time_start":scene_time_start,
                    "scene_time_end":scene_time_end,
                    "scene_duration":scene_duration,
                    "video_scene_file_path":img_path,
                    "procedure":procedure,
                    "base64_data":base64_data,
                }
            json_data_list.append(json_data) 
            i=i+1    
        video.release()

        audio_path = f"tmp_mp3/{base_video_path}.mp3"
        clip = VideoFileClip(video_path)
        clip.audio.write_audiofile(audio_path, bitrate="32k")
        clip.audio.close()
        clip.close()

        print(f"Extracted {len(base64Frames)} frames")
        print(f"Extracted audio to {audio_path}")
        return audio_path,json_data_list



# æ‰‹é †æ›¸è‡ªå‹•ä½œæˆ
def generate_procedure(llm_model,
                       temperature,
                       video_scene_file_list,
                       transcription_text):

    num_requests = len(video_scene_file_list)
    procedure_steps = []
    
    print("video_scene_file_list_len:" + str(len(video_scene_file_list)))

    llm = init_llm(llm_model,temperature)

    for i in range(num_requests):    
        frames_subset = [st.session_state["video_scene_file_list"][i]["base64_data"]]
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ 
        system_prompt = SystemMessage(content=f"ä¸ãˆã‚‰ã‚ŒãŸå‹•ç”»ãƒ‡ãƒ¼ã‚¿ã®ã‚·ãƒ¼ãƒ³{i+1}/{num_requests}ã¨æ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã‚’å…ƒã«ã€èª¬æ˜ã•ã‚Œã¦ã„ã‚‹ä½œæ¥­ã®æ‰‹é †ã‚’ã§ãã‚‹ã ã‘è©³ç´°ã«æ—¥æœ¬èªã§ç®‡æ¡æ›¸ãã§ä½œæˆã—ã¦ãã ã•ã„ã€‚")   
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        message = HumanMessage(
                        content=[
                            {"type":"text", "text":f"ã“ã¡ã‚‰ãŒå‹•ç”»ã‚·ãƒ¼ãƒ³{i+1}/{num_requests}ã®ãƒ•ãƒ¬ãƒ¼ãƒ ç”»åƒã§ã™ã€‚"},
                            *map(lambda x: {"type": "image_url", 
                                "image_url": {"url": f'data:image/jpg;base64,{x}', "detail": "low"}}, frames_subset),
                            {"type": "text", "text": f"éŸ³å£°ã®æ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™: {transcription_text}"}
                        ]
                    )
        
        response = llm.invoke([system_prompt,message]) 
        procedure = response.content
        procedure_steps.append(procedure)
        st.session_state["video_scene_file_list"][i]["procedure"] = procedure
        
        print(f"ã‚·ãƒ¼ãƒ³ {i+1}/{num_requests} ã®ä½œæ¥­æ‰‹é †:")
        print(procedure)
        print("=" * 40)  # åŒºåˆ‡ã‚Šç·šã‚’è¡¨ç¤º

    # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ 
    system_prompt = SystemMessage(content=f"""
ã“ã‚Œã¾ã§ã«ç”Ÿæˆã•ã‚ŒãŸå„ã‚·ãƒ¼ãƒ³ã®ä½œæ¥­æ‰‹é †ã‚’ã‚‚ã¨ã«ã€å‹•ç”»å…¨ä½“ã®è©³ç´°ãªä½œæ¥­æ‰‹é †æ›¸ã‚’ç« å»ºã¦ã¦æ§‹æˆã—ã€#å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ä½œæˆã—ã¦ãã ã•ã„ã€‚
å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®å‰å¾Œã«ä½™è¨ˆãªèª¬æ˜ã‚’ã¤ã‘ãªã„ã§ãã ã•ã„ã€‚
#å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:
|No|ä½œæ¥­å|è©³ç´°æ‰‹é †|
|xxx|xxxx|xxxx|
""")                                        
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    message = HumanMessage(
                    content=[
                        {"type":"text", "text":"\n".join(procedure_steps)},
                    ]
                )    
    final_response = llm.invoke([system_prompt,message])     

    main_container2.markdown(final_response.content,unsafe_allow_html=True)



# èª¬æ˜ç·¨é›†
@st.dialog("ğŸ“èª¬æ˜ç·¨é›†")
def delete_scene_note(target_no,scene_note):
    # ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
    update_scene_note = st.text_area(label="ğŸ“èª¬æ˜ç·¨é›†", 
                        value=scene_note,
                        key="input_scene_note",
                        height=500)

    scene_note_1,scene_note_2 = st.columns((1,1))
    if scene_note_1.button("æ›´æ–°"):
        st.session_state["video_scene_file_list"][target_no]["procedure"]  = update_scene_note 
        st.rerun()

    if scene_note_2.button("é–‰ã˜ã‚‹"):
        st.rerun()




# ã‚¿ãƒ–ã‚’ä½œæˆ
tab_titles = ['ğŸ’­é€šå¸¸ãƒãƒ£ãƒƒãƒˆ', 'ğŸ¥å‹•ç”»è§£æ']
tab1, tab2 = st.tabs(tab_titles)
# LLMãƒ¢ãƒ‡ãƒ«é¸æŠ
llm_model = st.sidebar.radio("ğŸ¤–LLM Model:",["gemini-1.5-flash","gemini-2.0-flash"], index=0)
temperature = st.sidebar.slider("ğŸŒ¡Temperature:", 0.0, 1.0, 0.0,step=0.1)
# ãƒãƒ£ãƒƒãƒˆã®ã‚¢ãƒã‚¿ãƒ¼
user_avatar = "ğŸ‘©â€ğŸ’»"
assistant_avatar = "ğŸ¤–"

     
# å„ã‚¿ãƒ–ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¿½åŠ 
with tab1:
    # ã‚¿ã‚¤ãƒˆãƒ«
    #st.markdown("### AI Chat") 
    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒŠ
    main_container1 = st.container(height=680)
    # ãƒœã‚¿ãƒ³ã‚³ãƒ³ãƒ†ãƒŠ
    buttons_container1 = st.container()
    # ãƒãƒ£ãƒƒãƒˆè¡¨ç¤º
    for message in st.session_state["messages"]:
        with main_container1.chat_message(
            message["role"],
            avatar=assistant_avatar if message["role"] == "assistant" else user_avatar,
        ):
            st.markdown(message["content"],unsafe_allow_html=True)
            
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ 
    system_prompt = SystemMessage(content="ã‚ãªãŸã¯è¦ªåˆ‡ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ—¥æœ¬èªã§ã“ãŸãˆã¦ãã ã•ã„ã€‚")
    # ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
    prompt = buttons_container1.text_area(label="ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", 
                                        placeholder="ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", 
                                        key="input_prompt_1",
                                        height=80)
    
    # ãƒœã‚¿ãƒ³
    col1,col2,col3,col4,col7 = buttons_container1.columns([1,1,1,4,1],vertical_alignment="bottom") 
    
    with col1:
        if st.button(label="ğŸ¨ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",key="button_upload_image",type="primary"):
            upload_image()                  
    with col3:
        # ãƒãƒ£ãƒƒãƒˆã«ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”»åƒã‚’è¡¨ç¤º
        if st.session_state["image_data"] != None:
            col3.image(st.session_state["image_data"],width=100)


    # å®Ÿè¡Œãƒœã‚¿ãƒ³ 
    if col2.button(label="ğŸ’­ãƒãƒ£ãƒƒãƒˆå®Ÿè¡Œ",key="button_exec_chat",type="primary"):
        if prompt == "":
            main_container1.error("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",icon="âœ–")
            st.stop()

        with main_container1.chat_message("user", avatar=user_avatar):  
            if st.session_state["base64_data"] != None:
                base64_data = st.session_state["base64_data"]
                disp_msg = f"""{prompt}<br><img src="data:image/jpeg;base64,{st.session_state["base64_data"]}"/>"""
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
                message = HumanMessage(
                    content=[
                        {
                            "type": "text",
                            "text": prompt,
                        },  
                        {
                            "type": "image_url",
                            "image_url": f"data:image/png;base64,${base64_data}",
                        },
                    ]
                )
                
                st.session_state["messages"].append({"role": "user", "content":  disp_msg})
                st.markdown(disp_msg,unsafe_allow_html=True) 
                        
                # AI    
                with main_container1.chat_message("assistant", avatar=assistant_avatar):
                    llm = init_llm(llm_model,temperature)
                    # LLMå®Ÿè¡Œ
                    with st.spinner(text="AIå®Ÿè¡Œä¸­...",show_time=True):
                        result = ""
                        response = llm.invoke([system_prompt,message]) 
                        result = response.content
                        st.session_state["messages"].append({"role": "assistant", "content": result})
                        st.markdown(result,unsafe_allow_html=True) 
            else:        
                disp_msg = f"""{prompt}"""
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
                message = HumanMessage(
                    content=[
                            {
                                "type": "text",
                                "text": prompt,
                            },  
                        ]
                )
                st.session_state["messages"].append({"role": "user", "content":  disp_msg})
                st.markdown(disp_msg,unsafe_allow_html=True)       
                # AI    
                with main_container1.chat_message("assistant", avatar=assistant_avatar):
                    llm = init_llm(llm_model,temperature)
                    # LLMå®Ÿè¡Œ
                    with st.spinner(text="AIå®Ÿè¡Œä¸­...",show_time=True):
                        response = llm.invoke([system_prompt,message]) 
                        result = response.content
                        st.session_state["messages"].append({"role": "assistant", "content": result})
                        st.markdown(result,unsafe_allow_html=True) 


    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚¯ãƒªã‚¢
    if col7.button("ğŸ†•æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ",key="button_chat_clear_1"):
        st.session_state["base64_data"] = None
        st.session_state["image_data"] = None
        st.session_state["messages"] = []
        st.rerun()




# å‹•ç”»è§£æ
with tab2:
    # ã‚¿ã‚¤ãƒˆãƒ«
    #st.markdown("### å‹•ç”»è§£æ")
    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒŠ
    main_container2 = st.container(height=680)
    # ãƒœã‚¿ãƒ³ã‚³ãƒ³ãƒ†ãƒŠ
    buttons_container2 = st.container()
    # ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
    prompt = buttons_container2.text_area(label="ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", 
                                        placeholder="ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", 
                                        key="prompt_input_2",
                                        height=80)

    # ãƒœã‚¿ãƒ³
    col1,col2,col3,col4,col7 = buttons_container2.columns([1,1,1,4,1],vertical_alignment="bottom") 

    with col1:
        if st.button(label="ğŸ¥å‹•ç”»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",key="button_upload_video",type="primary"):
            upload_video()  
    with col4:
        if st.session_state["tmp_audio_file_path"] != None:
            st.audio(st.session_state["tmp_audio_file_path"],autoplay=False)
      
    if len(st.session_state["video_scene_file_list"]) > 0:  
          
        st.session_state["anchor_ids"] = []   
        st.session_state["anchor_labels"] = []       
        st.session_state["anchor_icons"] = []             
        for json_data in st.session_state["video_scene_file_list"]:   
            st.session_state["anchor_ids"].append("scene_no_" + str(json_data["scene_no"]))
            st.session_state["anchor_labels"].append("ã‚·ãƒ¼ãƒ³NO:" + str(json_data["scene_no"]))
            st.session_state["anchor_icons"].append("ğŸ–¼ï¸")
         
        with st.sidebar:    
            st.subheader("ğŸ¥å‹•ç”»ã‚·ãƒ¼ãƒ³ä¸€è¦§")
            scroll_navbar(
                anchor_ids=st.session_state["anchor_ids"],
                anchor_labels=st.session_state["anchor_labels"],
                anchor_icons=st.session_state["anchor_icons"])              
                
        for i,anchor_id,anchor_label in zip(range(len(st.session_state["video_scene_file_list"])),st.session_state["anchor_ids"],st.session_state["anchor_labels"]):   
            main_container2.subheader(anchor_label,anchor=anchor_id)     
            sub_col1,sub_col2 = main_container2.columns((1,1))  
            # ã‚·ãƒ¼ãƒ³ç”»åƒè¡¨ç¤º
            with sub_col1:
                sub_col1.image(image=st.session_state["video_scene_file_list"][i]["video_scene_file_path"]) 
                sub_col1.button(label="ğŸ—‘ï¸ã‚·ãƒ¼ãƒ³å‰Šé™¤",key="scene_delete_" + anchor_id,on_click=delete_scene, args=(i, ))
            # ã‚·ãƒ¼ãƒ³èª¬æ˜è¡¨ç¤º
            with sub_col2:
                scene_text_container = sub_col2.container(border=1,height=450)
                scene_text_container.markdown(st.session_state["video_scene_file_list"][i]["procedure"],unsafe_allow_html=True)
                sub_col2.button(label="ğŸ“èª¬æ˜ç·¨é›†",key="scene_note_update_" + anchor_id,on_click=delete_scene_note, args=(i,st.session_state["video_scene_file_list"][i]["procedure"]))

 


    # å®Ÿè¡Œãƒœã‚¿ãƒ³ 
    if col2.button(label="ğŸ¥å‹•ç”»è§£æ",key="button_video",type="primary"):
    
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼            
        if len(st.session_state["video_scene_file_list"]) > 0:
            disp_msg = "å‹•ç”»è§£æã‚’è¡Œã„ã¾ã™ã€‚\n\n" + prompt

            # LLMå®Ÿè¡Œ
            with col3:
                with st.spinner(text="AIå®Ÿè¡Œä¸­...",show_time=True):  
                    # audioæ–‡å­—èµ·ã“ã—
                    transcription_text = ""
                    # ä½œæ¥­æ‰‹é †æ›¸ã‚’ç”Ÿæˆ
                    procedure = generate_procedure(llm_model=llm_model,
                                                    temperature=temperature,
                                                    video_scene_file_list=st.session_state["video_scene_file_list"] , 
                                                    transcription_text=transcription_text)
                    st.rerun()


        else:
            st.warning("å‹•ç”»ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")



    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚¯ãƒªã‚¢
    if col7.button("ğŸ†•ã‚¯ãƒªã‚¢",key="button_clear_2"):
        st.session_state["video_scene_file_list"] = []
        st.session_state["tmp_audio_file_path"] = None 
        st.session_state["anchor_ids"] = []
        st.session_state["anchor_labels"] = []
        st.session_state["anchor_icons"] = [] 
        
        shutil.rmtree("tmp_mp3/tmp")
        os.mkdir("tmp_mp3/tmp")       
        shutil.rmtree("tmp_video")
        os.mkdir("tmp_video")
        st.rerun()