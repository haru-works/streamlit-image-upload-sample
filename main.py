import os
import sys
import asyncio
import time
import base64
import streamlit as st
from PIL import Image
from pydantic import SecretStr
# langchain
from langchain_google_genai import ChatGoogleGenerativeAI 
from langchain_core.messages import HumanMessage,SystemMessage
# video
from moviepy.editor import VideoFileClip
import cv2
import numpy as np
from scenedetect import detect,open_video, AdaptiveDetector,SceneManager, split_video_ffmpeg
from scenedetect.detectors import ContentDetector
from scenedetect.video_splitter import split_video_ffmpeg
from scenedetect import VideoManager
from scenedetect import SceneManager
import tempfile
import math
# audio
#import whisper


# ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å«ã‚€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¾ã§ãƒ‘ã‚¹ã‚’é€šã™
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# ç’°å¢ƒå¤‰æ•°ã®èª­è¾¼
from dotenv import load_dotenv
load_dotenv()


# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="ãƒ‡ãƒ¢", page_icon="ğŸ¤©",layout="wide")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆ
if "base64_data" not in st.session_state:
    st.session_state["base64_data"] = None
    
if "image_data" not in st.session_state:
    st.session_state["image_data"] = None
     
if "video_scene_file_list" not in st.session_state:
    st.session_state["video_scene_file_list"] = []
    
if "base64_frames" not in st.session_state:
    st.session_state["base64_frames"] = None

if "tmp_audio_file_path" not in st.session_state:
    st.session_state["tmp_audio_file_path"] = None

if "uploaded_pic" in st.session_state and st.session_state["uploaded_pic"]:
    st.toast("ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ", icon="ğŸ“¥")
    del st.session_state["uploaded_pic"]

if "uploaded_video" in st.session_state and st.session_state["uploaded_video"]:
    st.toast("å‹•ç”»ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ", icon="ğŸ“¥")
    del st.session_state["uploaded_video"]

if "messages" not in st.session_state:
    st.session_state["messages"] = []


# llmåˆæœŸåŒ–
def init_llm(llm_model: str):
    if llm_model == 'gemini':
        return ChatGoogleGenerativeAI(model="gemini-1.5-flash",
                                      api_key=SecretStr(os.getenv('GOOGLE_API_KEY')),
                                      temperature=0,)

    else:
        st.error(f'ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ãªã„LLMãƒ¢ãƒ‡ãƒ«ã§ã™: {llm_model}',icon="âœ–")


# ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
@st.dialog("ğŸ¨ ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
def upload_image():
    st.warning("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸã„ç”»åƒã‚’é¸æŠã—ã¦ãã ã•ã„",icon="ğŸ’¡",)
    picture = st.file_uploader("ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ", 
                               type=["jpg", "png", "bmp"], 
                               label_visibility="hidden") 
    
    # ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    if picture:   
        
        # ç”»åƒã‚’ãƒã‚¤ãƒŠãƒªã«å¤‰æ›
        bytes_data = picture.getvalue()
        # ç”»åƒã‚’è¡¨ç¤º
        image = Image.open(picture)
        st.session_state["uploaded_pic"] = True
        st.image(image, caption='ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼') 
        
        #ã€€ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³æŠ¼ã™
        if st.button("ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"):
            # ç”»åƒã‚’base64ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            base64_data = base64.b64encode(bytes_data).decode('utf-8')
            st.session_state["base64_data"] = base64_data
            st.session_state["image_data"] = image
            st.rerun()


# å‹•ç”»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
@st.dialog("ğŸ¥ å‹•ç”»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
def upload_video():
    st.warning("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸã„å‹•ç”»ã‚’é¸æŠã—ã¦ãã ã•ã„",icon="ğŸ’¡",)
    video = st.file_uploader("ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ", 
                               type=["mp4", "avi"], 
                               label_visibility="hidden") 
    
    # å‹•ç”»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    if video:   
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        with tempfile.NamedTemporaryFile(delete=False, suffix=video.name) as temp_file:
            temp_file.write(video.read())
            temp_file_path = temp_file.name
            # å‹•ç”»ã‚’è¡¨ç¤º
            st.session_state["uploaded_video"] = True
            st.video(video) 
            # ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³æŠ¼ã™
            if st.button("ã‚·ãƒ¼ãƒ³æ¤œå‡º"):
                base64Frames, audio_path ,json_data_list = split_video_into_scenes(video_path=temp_file_path,threshold=30)
                #base64Frames, audio_path = process_video(temp_file_path,2)
                st.session_state["base64_frames"] = base64Frames
                st.session_state["tmp_audio_file_path"] = audio_path
                st.session_state["video_scene_file_list"] = json_data_list
                st.rerun()


# å‹•ç”»ãƒ•ãƒ¬ãƒ¼ãƒ è§£æ
def split_video_into_scenes(video_path, threshold=27.0):
    with st.spinner("ã‚·ãƒ¼ãƒ³æ¤œå‡ºä¸­â€¦",show_time=True):
        # VideoManager ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        video_manager = VideoManager([video_path])
        scene_manager = SceneManager()
    
        # ãƒ“ãƒ‡ã‚ªã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆã‚’å–å¾—
        framerate = video_manager.get_framerate()

        # # ContentDetectorã‚’é–¾å€¤ã€æœ€å°ã‚·ãƒ¼ãƒ³é•·ã€ãƒ•ãƒ¬ãƒ¼ãƒ ã‚¹ã‚­ãƒƒãƒ—ã¨å…±ã«è¿½åŠ 
        scene_manager.add_detector(ContentDetector(threshold))

        # ãƒ“ãƒ‡ã‚ªã¨ã‚·ãƒ¼ãƒ³ãƒãƒãƒ¼ã‚¸ãƒ£ã‚’é–¢é€£ä»˜ã‘
        video_manager.set_downscale_factor()
        video_manager.start()
        scene_manager.detect_scenes(frame_source=video_manager)

        # æ¤œå‡ºã•ã‚ŒãŸã‚·ãƒ¼ãƒ³ã®ãƒªã‚¹ãƒˆã‚’å–å¾—
        scene_list = scene_manager.get_scene_list(base_timecode=video_manager.get_base_timecode(),start_in_scene=True)
        # å„ã‚·ãƒ¼ãƒ³ã®æœ€åˆã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç”»åƒã¨ã—ã¦ä¿å­˜ã—ã€ã‚·ãƒ¼ãƒ³ã®ç§’æ•°ã‚’è¨ˆç®—
        cap = cv2.VideoCapture(video_path)
        for i, scene in enumerate(scene_list):
            start_frame = scene[0].get_frames()
            end_frame = scene[1].get_frames()
            scene_duration = (end_frame - start_frame) / framerate
            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
            ret, frame = cap.read()
            base64Frames = []
            if ret:
                img_path = f'tmp_video/scene_{i+1}.jpg'
                scene_no = i+1
                scene_time_start = scene[0].get_timecode()
                scene_time_end = scene[1].get_timecode()
                _, buffer = cv2.imencode(".jpg", frame) # æ–‡å­—åˆ—ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
                base64_data = (base64.b64encode(buffer).decode("utf-8"))
                base64Frames.append(base64_data)
                json_data_list = []
                cv2.imwrite(img_path, frame)
                json_data = {
                    "scene_no":scene_no,
                    "scene_time_start":scene_time_start,
                    "scene_time_end":scene_time_end,
                    "scene_duration":scene_duration,
                    "video_scene_file_path":img_path,
                    "base64_data":base64_data
                }
                json_data_list.append(json_data)
            cap.release()
            
        # Extract audio from video
        audio_path = f"tmp_mp3/{video_path}.mp3"
        clip = VideoFileClip(video_path)
        clip.audio.write_audiofile(audio_path, bitrate="32k")
        clip.audio.close()
        clip.close()

    print(f"Extracted {len(base64Frames)} frames")
    print(f"Extracted audio to {audio_path}")
    return base64Frames, audio_path, json_data_list



# å‹•ç”»ãƒ•ãƒ¬ãƒ¼ãƒ è§£æ&éŸ³å£°æŠ½å‡º
def process_video(video_path, seconds_per_frame=2):
    with st.spinner("ã‚·ãƒ¼ãƒ³æ¤œå‡ºä¸­â€¦",show_time=True):
        base64Frames = []
        base_video_path, _ = os.path.splitext(video_path)

        video = cv2.VideoCapture(video_path)
        total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = video.get(cv2.CAP_PROP_FPS)
        print("fps=", fps)
        frames_to_skip = int(fps * seconds_per_frame)
        curr_frame=0

        while curr_frame < total_frames - 1:
            video.set(cv2.CAP_PROP_POS_FRAMES, curr_frame)
            success, frame = video.read()
            if not success:
                break
            _, buffer = cv2.imencode(".jpg", frame)
            base64Frames.append(base64.b64encode(buffer).decode("utf-8"))
            curr_frame += frames_to_skip
        video.release()

        audio_path = f"tmp_mp3/{base_video_path}.mp3"
        clip = VideoFileClip(video_path)
        clip.audio.write_audiofile(audio_path, bitrate="32k")
        clip.audio.close()
        clip.close()

        print(f"Extracted {len(base64Frames)} frames")
        print(f"Extracted audio to {audio_path}")
        return base64Frames, audio_path


# # éŸ³å£°è§£æ streamlitã ã¨ä½¿ãˆãªã„ APIåŒ–å¿…è¦
# async def transcribe_audio(audio_path):
#     model = whisper.load_model("medium")  # ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã™ã‚‹
#     #start_time = time.time()
#     result = asyncio.run(model.transcribe(audio_path, fp16=False))  # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®æ–‡å­—èµ·ã“ã—
#     #end_time = time.time()
#     print(result["text"])  # æ–‡å­—èµ·ã“ã—çµæœã®è¡¨ç¤º
#     #print(f'å®Ÿè¡Œæ™‚é–“: {end_time - start_time} seconds')
#     return result["text"]


# æ‰‹é †æ›¸è‡ªå‹•ä½œæˆ
def generate_procedure(llm_model,base64_frames, transcription_text, max_frames_per_request=30):
    num_requests = math.ceil(len(base64_frames) / max_frames_per_request)
    procedure_steps = []

    llm = init_llm(llm_model)

    for i in range(num_requests):
        start_index = i * max_frames_per_request
        end_index = min((i + 1) * max_frames_per_request, len(base64_frames))
        frames_subset = base64_frames[start_index:end_index]

        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ 
        system_prompt = SystemMessage(content=f"ä¸ãˆã‚‰ã‚ŒãŸå‹•ç”»ãƒ‡ãƒ¼ã‚¿ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ{i+1}/{num_requests}ã¨æ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã‚’å…ƒã«ã€èª¬æ˜ã•ã‚Œã¦ã„ã‚‹ä½œæ¥­ã®æ‰‹é †ã‚’ã§ãã‚‹ã ã‘è©³ç´°ã«æ—¥æœ¬èªã§ç®‡æ¡æ›¸ãã§ä½œæˆã—ã¦ãã ã•ã„ã€‚")   
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        message = HumanMessage(
                        content=[
                            {"type":"text", "text":f"ã“ã¡ã‚‰ãŒå‹•ç”»ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ{i+1}/{num_requests}ã®ãƒ•ãƒ¬ãƒ¼ãƒ ç”»åƒã§ã™ã€‚"},
                            *map(lambda x: {"type": "image_url", 
                                "image_url": {"url": f'data:image/jpg;base64,{x}', "detail": "low"}}, frames_subset),
                            {"type": "text", "text": f"éŸ³å£°ã®æ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™: {transcription_text}"}
                        ]
                    )
        response = llm.invoke([system_prompt,message]) 
        procedure = response.content
        procedure_steps.append(procedure)

        print(f"ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ {i+1}/{num_requests} ã®ä½œæ¥­æ‰‹é †:")
        print(procedure)
        print("=" * 40)  # åŒºåˆ‡ã‚Šç·šã‚’è¡¨ç¤º
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ 
        system_prompt = SystemMessage(content=f"""
ã“ã‚Œã¾ã§ã«ç”Ÿæˆã•ã‚ŒãŸå„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ä½œæ¥­æ‰‹é †ã‚’ã‚‚ã¨ã«ã€å‹•ç”»å…¨ä½“ã®è©³ç´°ãªä½œæ¥­æ‰‹é †æ›¸ã‚’ç« å»ºã¦ã¦æ§‹æˆã—ã€#å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ä½œæˆã—ã¦ãã ã•ã„ã€‚
å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®å‰å¾Œã«ä½™è¨ˆãªèª¬æ˜ã‚’ã¤ã‘ãªã„ã§ãã ã•ã„ã€‚
#å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:
|No|ä½œæ¥­å|è©³ç´°æ‰‹é †|
|xxx|xxxx|xxxx|
""")                                        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        message = HumanMessage(
                        content=[
                            {"type":"text", "text":"\n".join(procedure_steps)},
                        ]
                    )    
        final_response = llm.invoke([system_prompt,message])     
        
    return final_response.content


# ãƒãƒ£ãƒƒãƒˆã®ã‚¢ãƒã‚¿ãƒ¼
user_avatar = "ğŸ‘©â€ğŸ’»"
assistant_avatar = "ğŸ¤–"

# ã‚¿ã‚¤ãƒˆãƒ«
st.markdown("### AI Chat Test")

# LLMãƒ¢ãƒ‡ãƒ«é¸æŠ
llm_model = st.sidebar.radio("LLMãƒ¢ãƒ‡ãƒ«é¸æŠ:",["gemini"], index=0)

# ãƒãƒ£ãƒƒãƒˆè¡¨ç¤ºã‚³ãƒ³ãƒ†ãƒŠ
main_container = st.container(height=500)

# ãƒãƒ£ãƒƒãƒˆè¡¨ç¤º
for message in st.session_state["messages"]:
    with main_container.chat_message(
        message["role"],
        avatar=assistant_avatar if message["role"] == "assistant" else user_avatar,
    ):
        st.markdown(message["content"],unsafe_allow_html=True)

if st.session_state["video_scene_file_list"] != []:
    for json_data in st.session_state["video_scene_file_list"]:
        disp_msg = f"""scene_no{json_data["scene_no"]} é–‹å§‹{json_data["scene_time_start"]} çµ‚äº†{json_data["scene_time_end"]} å‹•ç”»ã®é•·ã•{json_data["scene_duration"]} <br><img  width="300" height="200" src="data:image/jpeg;base64,{json_data["base64_data"]}"/>"""
        main_container.markdown(disp_msg,unsafe_allow_html=True)
 
if st.session_state["tmp_audio_file_path"] != None:
    main_container.audio(st.session_state["tmp_audio_file_path"])
      
        
# ãƒœã‚¿ãƒ³ã‚³ãƒ³ãƒ†ãƒŠ
buttons_container = st.container()

# ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
prompt = buttons_container.text_area(label="ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", 
                                     placeholder="æ±äº¬ã®æ˜æ—¥ã®å¤©æ°—ã‚’æ—¥æœ¬èªã§æ•™ãˆã¦", 
                                     height=90)

# åˆ©ç”¨ã—ãªã„å ´åˆ
col1,col2,col3,col4,col5 = buttons_container.columns([2,1.2,1.2,9,3.8],
                                                     vertical_alignment="bottom") 
with col2:
    if st.button(label="ğŸ¨"):
        upload_image()
with col3:
    if st.button(label="ğŸ¥"):
        upload_video()                     
with col4:
    if st.button(label="ğŸ—‘ï¸"):
        st.session_state["image_data"] = None
        st.session_state["base64_data"] = None
        st.session_state["video_scene_file_list"] = []
        st.session_state["base64_frames"] = None
        st.session_state["tmp_audio_file_path"] = None
        st.rerun()
with col5:
    # ãƒãƒ£ãƒƒãƒˆã«ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”»åƒã‚’è¡¨ç¤º
    if st.session_state["image_data"] != None:
        col5.image(st.session_state["image_data"],width=100)
 
# å®Ÿè¡Œãƒœã‚¿ãƒ³ 
if col1.button(label="å®Ÿè¡Œ",type="primary"):
  
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼
    with main_container.chat_message("user", avatar=user_avatar):  
        if st.session_state["base64_data"] != None:
            base64_data = st.session_state["base64_data"]
            disp_msg = f"""{prompt}<br><img src="data:image/jpeg;base64,{st.session_state["base64_data"]}"/>"""
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ 
            system_prompt = SystemMessage(content="ã‚ãªãŸã¯è¦ªåˆ‡ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ—¥æœ¬èªã§ã“ãŸãˆã¦ãã ã•ã„ã€‚")
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            message = HumanMessage(
                content=[
                    {
                        "type": "text",
                        "text": prompt,
                    },  
                    {
                        "type": "image_url",
                        "image_url": f"data:image/png;base64,${base64_data}",
                    },
                ]
            )
             
            st.session_state["messages"].append({"role": "user", "content":  disp_msg})
            st.markdown(disp_msg,unsafe_allow_html=True) 
                    
            # AI    
            with main_container.chat_message("assistant", avatar=assistant_avatar):
                llm = init_llm(llm_model)
                # LLMå®Ÿè¡Œ
                with st.spinner(text="AIå®Ÿè¡Œä¸­...",show_time=True):
                    result = ""
                    response = llm.invoke([system_prompt,message]) 
                    result = response.content
                    st.session_state["messages"].append({"role": "assistant", "content": result})
                    st.markdown(result,unsafe_allow_html=True) 

                          
        elif st.session_state["tmp_audio_file_path"] != None and st.session_state["base64_frames"] !=None:
            disp_msg = "å‹•ç”»ã‹ã‚‰æ‰‹é †æ›¸ä½œæˆã‚’è¡Œã„ã¾ã™"
            st.session_state["messages"].append({"role": "user", "content":  disp_msg})
            st.markdown(disp_msg,unsafe_allow_html=True) 
            if st.session_state["video_scene_file_list"] != []:
                for json_data in st.session_state["video_scene_file_list"]:
                    disp_msg = f"""scene_no{json_data["scene_no"]} é–‹å§‹{json_data["scene_time_start"]} çµ‚äº†{json_data["scene_time_end"]} å‹•ç”»ã®é•·ã•{json_data["scene_duration"]} <br><img  width="300" height="200" src="data:image/jpeg;base64,{json_data["base64_data"]}"/>"""
                    main_container.markdown(disp_msg,unsafe_allow_html=True)
                    st.session_state["messages"].append({"role": "user", "content":  disp_msg})
                    
                        
            
            # LLMå®Ÿè¡Œ
            with st.spinner(text="AIå®Ÿè¡Œä¸­...",show_time=True):  
                # é–¢æ•°ã‚’å®Ÿè¡Œã—ã¦æ–‡å­—èµ·ã“ã—ã‚’è¡Œã†
                #transcription_text = asyncio.run(transcribe_audio(st.session_state["tmp_audio_file_path"]))
                transcription_text = ""
                # ä½œæ¥­æ‰‹é †æ›¸ã‚’ç”Ÿæˆ
                procedure = generate_procedure(llm_model,st.session_state["base64_frames"] , transcription_text, max_frames_per_request=34)
                #å›ç­”çµæœã®è¡¨ç¤º
                st.session_state["messages"].append({"role": "assistant", "content": procedure})
                st.markdown(procedure,unsafe_allow_html=True) 
              
        else:
            if prompt == "":
                main_container.error("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",icon="âœ–")
                time.sleep(2)
                st.rerun()
                
            disp_msg = f"""{prompt}"""
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ 
            system_prompt = SystemMessage(content="ã‚ãªãŸã¯è¦ªåˆ‡ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ—¥æœ¬èªã§ã“ãŸãˆã¦ãã ã•ã„ã€‚")
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            message = HumanMessage(
                content=[
                        {
                            "type": "text",
                            "text": prompt,
                        },  
                    ]
            )
            st.session_state["messages"].append({"role": "user", "content":  disp_msg})
            st.markdown(disp_msg,unsafe_allow_html=True)       
            # AI    
            with main_container.chat_message("assistant", avatar=assistant_avatar):
                llm = init_llm(llm_model)
                # LLMå®Ÿè¡Œ
                with st.spinner(text="AIå®Ÿè¡Œä¸­...",show_time=True):
                    result = ""
                    response = llm.invoke([system_prompt,message]) 
                    result = response.content
                    st.session_state["messages"].append({"role": "assistant", "content": result})
                    st.markdown(result,unsafe_allow_html=True) 
                    
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¯ãƒªã‚¢       
        st.session_state["base64_data"] = None
        st.session_state["image_data"] = None
        st.session_state["video_scene_file_list"] = []
        st.session_state["base64_frames"] = None
        st.session_state["tmp_audio_file_path"] = None
        st.rerun()

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚¯ãƒªã‚¢
if col5.button("æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ"):
    st.session_state["base64_data"] = None
    st.session_state["image_data"] = None
    st.session_state["messages"] = []
    st.session_state["video_scene_file_list"] = []
    st.session_state["base64_frames"] = None
    st.session_state["tmp_audio_file_path"] = None 
    st.rerun()